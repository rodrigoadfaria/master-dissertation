%% ------------------------------------------------------------------------- %%
\chapter{Theoretical Background}
\label{cap:conceitos}
The image analysis is one of the most important tasks in a computer vision system. Its goal is to create a suitable description with enough information to differentiate the objects in the scene. In general, this description is typically based on shapes, textures, gray levels or color of those objects in the image. With this description, useful interpretation can be extracted from the image by means of an automatic computer system that facilitates human perception.

There is no general agreement among authors regarding where image processing stops and computer vision starts. The first, as the title says, processes the image by applying some transformations on it such that smoothing, sharpening, noise reduction, lightening enhancement, contrasting, stretching, and compression. These will result on a more enhanced and readable image. In addition, the input and output of the process are always images. On the other hand, computer vision has the ultimate goal to use computers to emulate human vision, including learning and being able to make inferences and take actions based on visual inputs \citep{gonzalez:02}. In general, computer vision systems benefit from image processing techniques as pre-processing steps to build better applications. Thus, we can see that they definitely are not different fields, but there is an overlapping between them.

Once this work is intended to explore new methods on human skin detection, we will use techniques from both fields. Color space transformation from image processing, for example, as well as human skin segmentation and understanding as part of computer vision. This is a tentative to imitate the human visual system and its capability to recognize others from the same specie -- of course, humans use other characteristics to identify other humans like shape, high, gender, and others, but skin is also part of this recognition system.

\textcolor{red}{Fix this at the end of chapter definition} Therefore, in this chapter, the theoretical concepts that apply to this research are stated. First, a short introduction to color models is provided in order to give an overview of the main characteristics of some of the most used in the computer vision and image processing area, on which this research is based. Thereafter, some machine learning methods are defined and explained, once they were used in the preliminary experiments of this work.

%% ------------------------------------------------------------------------- %%
\section{Digital image}
\label{sec:digital_image}
By definition, an image is a two-dimensional function $f(x, y)$, where $x$ and $y$ are spatial coordinates, and the amplitude of $f$ at any pair of coordinates $(x, y)$ is called the \textit{intensity} or \textit{gray level} of the image at that point. The image is said digital when the function $f(x, y)$ is converted to a discrete form. This is made by a process called \textit{digitalization}, which consists of two steps: \textit{sampling} and \textit{quantization} \citep{gonzalez:02}.

Each element of the discrete function $f(x, y)$ is called \textit{pixel} (picture element), where $0 \leq x \leq W - 1$ and $0 \leq y \leq H - 1$. This means that the image can be represented in a matrix form (see Eq.~\ref{eq:image_function}), where $W$ is the number of lines and $H$ the number of columns of the matrix. Therefore, $W$ and $H$ defines the size or resolution of the image \citep{pedrini:08}.

\begin{equation*}
f(x, y) =
 \begin{bmatrix}
  f(0, 0)     & f(0, 1)     & \cdots & f(0, H - 1) \\
  f(1, 0)     & f(1, 1)     & \cdots & f(1, H - 1) \\
  \vdots      & \vdots      & \ddots & \vdots  \\
  f(W - 1, 0) & f(W - 1, 1) & \cdots & f(W - 1, H - 1)
 \end{bmatrix}
\label{eq:image_function}
\end{equation*}

Usually, pixels are stored, and therefore read, in this matrix in an order known as \textit{raster}. This information is important so that capture and display devices can be able to establish a common interface, and make necessary transformations in the coordinates, when needed.

In a monochromatic digital image, the value of a pixel is a scalar in the range $[L_{min}, L_{max}]$, where $L$ is the (integer) number of gray-levels~\citep{pedrini:08}.

In a multispectral image, each pixel has a vector value such that $f(x, y) = (L_1, L_2, \ldots L_n)$ where $L_{min} \leq L_i \leq L_{max}$ and $n = 1, 2, 3, \ldots, n$. In general, $L_i$ can represent different measures for each of $(x, y)$ coordinate as well as different intervals~\citep{pedrini:08}.

A colored image is a multispectral image, where the color in each $(x, y)$ point is given by three variables: brightness, hue and saturation \citep{pedrini:08}. The brightness gives the notion of chromatic intensity. Hue represents the dominant color perceived by an observer. Saturation refers to the relative purity or amount of white light applied to the hue. Combined, hue and saturation are known as chromaticity and, therefore, a color must be characterized by its brightness and chromaticity \citep{gonzalez:02}.


%% ------------------------------------------------------------------------- %%
\section{Basic relationship between pixels}
\label{sec:image_components_relation}
There is a number of applications in image processing and computer vision that uses information of relationship among pixels to create knowledge. Some of these important relationships will be described in the following sections once we will apply it in further chapter~\ref{cap:proposed-solution}. It's worth mentioning that we defined an image as a function $f(x, y)$. In this section, when referring to a particular pixel we will denote it in lowercase letters such as $p$.



%% ------------------------------------------------------------------------- %%
\subsection{Neighborhood}
\label{sec:neighborhood}
A pixel $p$ with coordinates $(x, y)$ has four horizontal and vertical neighbors whose coordinates are given by:

\begin{equation*}
    (x + 1, y), (x - 1, y), (x, y + 1), (x, y - 1)
    \label{eq:n4_neighbors}
\end{equation*}

This set of pixels, called the 4-\textit{neighbors} of $p$, is denoted by $N_4(p)$ \citep{gonzalez:02}. See figure~\ref{fig:n4-neighbors} for a reference on how this neighborhood looks like.

\begin{figure}[ht]
    \centering
    \begin{tikzpicture}
    \draw[step=0.75cm,black,thin] (0,0) grid (0.75,2.25);
    \draw[black,thin] (0.75,0.75) rectangle (1.5,1.5);
    \draw[black,thin] (-0.75,0.75) rectangle (0,1.5);
    \fill[gray!40, draw=black] (0,0.75) rectangle (0.75,1.5);
    
    \end{tikzpicture}

    \caption[The 4-neighbors representation of a pixel $p$]{The 4-neighbors representation of a pixel $p$. The pixel $p$ is centered on the grid with gray background.}
    \label{fig:n4-neighbors}
\end{figure}

Each pixel of the image is a unite distance from $(x, y)$. Some neighbors of $p$ might lie outside of the image boundaries if $(x, y)$ is on the border of the image \citep{gonzalez:02}. Those who will use neighborhood operations in the image might take this in consideration to avoid index out of range in those areas.

The four coordinates of the diagonals of $p$ are given by:
\begin{equation*}
    (x + 1, y + 1), (x + 1, y - 1), (x - 1, y + 1), (x - 1, y - 1)
    \label{eq:nd_neighbors}
\end{equation*}

This set of pixels are denoted by $N_D(p)$. When combined, the 4-\textit{neighbors} and $N_D(p)$ will generate the 8-\textit{neighbors} of $p$, known as $N_8(p)$ \citep{gonzalez:02}. Formally, we have:
\begin{equation*}
    N_8(p) = N_4(p) \cup N_D(p)
    \label{eq:n8_neighbors}
\end{equation*}

See figure~\ref{fig:n8-neighbors} for a reference on how the $N_8(p)$ neighborhood looks like.

\begin{figure}[ht]
    \centering
    \begin{tikzpicture}
    \draw[step=0.75cm,black,thin] (0,0) grid (2.25,2.25);
    \fill[gray!40, draw=black] (0.75,0.75) rectangle (1.5,1.5);
    \end{tikzpicture}

    \caption[The 8-neighbors representation of a pixel $p$]{The 8-neighbors representation of a pixel $p$. The pixel $p$ is centered on the grid with gray background.}
    \label{fig:n8-neighbors}
\end{figure}

Despite these are the most common neighbors used in applications, other different distances from $p$ as well as connectivity can be applied. The idea of neighborhood can also be extended to 3-dimensional images where, instead of pixels, the voxels are the coordinates considered.


%% ------------------------------------------------------------------------- %%
\subsection{Connectivity}
\label{sec:connectivity}

%% ------------------------------------------------------------------------- %%
\section{Image segmentation}
\label{sec:image_segmentation}

%% ------------------------------------------------------------------------- %%
\section{Pattern recognition}
\label{sec:pattern_recognition}

%% ------------------------------------------------------------------------- %%
\section{Color models}
\label{sec:color_models}

The use of color images in computer vision or image processing can be motivated by two main factors. The first refers to the powerful characteristic of color to function as a descriptor that often simplifies the identification and extraction of an object in a scene. The second is related to the ability of humans to discern thousands of tonalities and intensities compared to only a few dozen levels of gray \citep{gonzalez:02}.

The visual perception of color by the human eye should not vary according to the spectral distribution of the natural light incident upon an object. In other words, the color appearance of objects remains stable under different lighting conditions. This phenomenon is known as color constancy \citep{gevers:12}.

As an example, the grass of a soccer stadium remains green throughout the day, even at dusk when, from a physical point of view, sunlight has a more reddish appearance.

The human perception of colors occurs by the activation of nerve cells that send signals to the brain about brightness, hue and saturation, which are usually the features used to distinguish one color from another \citep{gonzalez:02}.

The brightness gives the notion of chromatic intensity. Hue represents the dominant color perceived by an observer. Saturation refers to the relative purity or amount of white light applied to the hue. Combined, hue and saturation are known as chromaticity and, therefore, a color must be characterized by its brightness and chromaticity \citep{gonzalez:02}.

Colors can be specified by mathematical models in tuples of numbers in a coordinate system and a subspace within that system where each color is represented by a single point. Such models are known as the color models \citep{gonzalez:02}.

These models can be classified as of two types: the additive models in which the primary color intensities are added to produce other colors and subtractive, where colors are generated by subtracting the length of the dominant wave from the white light.

The following sections briefly describe some of the major color models, as well as their variants and main areas of application.

%% ------------------------------------------------------------------------- %%
\subsection{Munsell color model}
\label{sec:modelo_cores_munsell}

Pioneer in an attempt to organize the perception of color in a color space, Albert H. Munsell was able to combine the art and science of colors in a single theory \citep{konstantinos:00}.

The principle of equality of visual spacing between the components of the model is the essential idea of the Munsell color model. These components are hue, value, corresponding to luminance, and chroma, corresponding to saturation \citep{konstantinos:00}.

The model is represented by a cylindrical shape and it can be seen in the figure ~\ref{fig:munsell-system}. The hue is arranged in the circular axis consisting of five base as well as five secondary colors, the saturation in the radial axis and the luminance in the vertical axis in a range varying from 0 to 10.

\begin{figure}[!ht]
  \centering
  \includegraphics[width=.55\textwidth]{munsell-system}
  % fonte https://commons.wikimedia.org/wiki/File:Munsell-system.svg
  \caption[Munsell color model.]{Munsell color model represented by a cylindrical shape. The hue is arranged on the circular axis consisting of five base and five secondary colors, the saturation on the radial axis and the luminance on the vertical axis in a range varying from 0 to 10. Source: \citet{rus:07}.}
  \label{fig:munsell-system} 
\end{figure}

%% ------------------------------------------------------------------------- %%
\subsection{CIE color model}
\label{sec:modelo_cores_cie}

In 1931, the CIE established the first mathematical model of color numerical specification, whose objective was to analyze the relationship between the physical aspects of colors in the electromagnetic spectrum and their perception by the human visual system to determine how an ordinary person perceives the color. A review of this specification was published in 1964 \citep{gonzalez:02}.

The experiment that originated the standard consisted in detecting the colors perceived by an observer from a mixture of three primary colors X, Y and Z called tristimulus values. These coordinates gave rise to the CIE XYZ color space which encompasses all the colors that can be perceived by an ordinary human being. For this reason, it is considered an device independent representation \citep{konstantinos:00}.

The system proposed by the CIE XYZ to describe a color is based on a luminance component Y, and two additional components X and Z, that bring the chromaticity information. This system is formed by imaginary colors that can be expressed as combinations of the normalized measures shown in the equations~\ref{eq:cie_x}, \ref{eq:cie_y} and \ref{eq:cie_z}.

\begin{equation}
  x = \frac{X}{X + Y + Z}
\label{eq:cie_x}
\end{equation}

\begin{equation}
  y = \frac{Y}{X + Y + Z}
\label{eq:cie_y}
\end{equation}

\begin{equation}
  z = \frac{Z}{X + Y + Z}
\label{eq:cie_z}
\end{equation}

where $x + y+ z = 1$.

Combinations of negative values and other problems related to selecting a set of real primaries are eliminated. The chromaticity coordinates $x$ and $y$ allow to represent all colors in a two-dimensional plane, also known as a chromaticity diagram, which can be seen in the figure ~\ref{fig:cie-cromaticity-diagram}.

\begin{figure}[!ht]
  \centering
  \includegraphics[width=.5\textwidth]{cie-cromaticity-diagram}
  % fonte https://en.wikipedia.org/wiki/File:CIE1931xy_blank.svg
  \caption[CIE 1931 chromaticity diagram]{CIE 1931 chromaticity diagram. The points representing pure colors in the electromagnetic spectrum are labeled according to their wavelengths and are located along the curve from the right end of the x-axis, corresponding to the red color, to the left end of the same axis, corresponding to the violet color, forming a polygon similar to a horseshoe. The internal points correspond to all possible combinations of visible colors. Source: \citet{ben:09}.}
  \label{fig:cie-cromaticity-diagram} 
\end{figure}

The coordinates $ (x = 1/3, y = 1/3) $ correspond to the location of white light, also known as white point, and serve as reference in the process of image capture, coding, or reproduction.

CIE also derived and standardized two other color models based on CIE XYZ specification and, likewise, are device independent. Both are perceptually uniform, which means that equal perceptual distances separate all colors in the system~\citep{vezhnevets:03}. As an example, the gray scale of the space should allow for a smooth transition between black and white.

The first one was designed to reduce the problem of perceptual non-uniformity. Some Uniform Chromaticity Scale (UCS) diagrams were proposed based on mathematical equations to transform the values XYZ or the coordinates $x, y$ into a new set of values $(u, v)$, which gave rise to the 1960 CIE $uv$ chromaticity diagram~\citep{gevers:12}.

Still with unsatisfactory results, the CIE made a new change by multiplying the $v$ component by a factor of 1.5. In addition, the brightness scale given by the Y component has been replaced by $L^* = [0, 100]$ to better represent the differences in luminosity that are equivalent. This revision originated the CIE 1976 $L^*u^*v^*$ color model, commonly known by the acronym CIELuv~\citep{gevers:12}.

In 1976 the CIE adopted a new color model, based on the $L, a, b$ model, proposed by Richard Hunter in 1948, which best represented the uniform spacing of colors. Named CIE $L^*a^*b^*$ and known by the acronym CIELab, it is a space based on opponent colors \footnote{Theory started around 1500 when Leonardo da Vinci concluded that colors are produced by mixing yellow and blue, green and red, and white and black. In 1950, this theory was confirmed when optically-colored signals were detected at the optical connection between the retina and the brain~\citep{gevers:12}.} in which the color stimuli of retina is converted to distinctions between light and dark, red and green, and blue and yellow, represented by the axes $L^*$, $a^*$, and $b^*$, respectively~\citep{gevers:12}.


%% ------------------------------------------------------------------------- %%
\subsection{RGB color model}
\label{sec:modelo_cores_rgb}

The RGB model, an acronym for Red, Green, and Blue, is an additive color model in which the three primary colors red, green and blue are added to produce the others \citep{gonzalez:02}.

This system was based on the trichromatic theory of Thomas Young and Hermann Helmholtz in the mid-19th century and can be represented graphically
through the unit cube defined on the axes R, G and B, as illustrated in the figure~\ref{fig:rgb-cube} \citep{konstantinos:00}.

\begin{figure}[!ht]
  \centering
  \includegraphics[width=.35\textwidth]{rgb-cube}
  % http://www.scratchapixel.com/old/lessons/3d-basic-lessons/lesson-5-colors-and-digital-images/color-spaces/
  \caption[Unit cube representing the colors of the RGB model]{Unit cube representing the colors of the RGB model. The origin, given by the vertex $(0, 0, 0)$, represents the black color. The vertex $(1, 1, 1)$, opposite the origin, represents the white color. The highlighted vertices on the axes represent the primary colors and the others are the complement of each. Each point inside the cube corresponds to a color that can be represented by the triple $(r, g, b)$, where $r, g, b \in [0, 1]$. The shades of gray are represented along the main diagonal of the cube, with each point along this diagonal being formed by equal contributions of each primary color. Source: adapted from \citet{gonzalez:02}.}
  \label{fig:rgb-cube} 
\end{figure}

It is noteworthy that there are two ways of representing the RGB space: linear and non-linear. The above-mentioned system shows the non-linear model, whose abbreviation is $R'G'B'$, and is most used by devices and applications because of their similarity to the human visual system. In the literature, this system is frequently cited with the acronym RGB, which makes the nomenclature dubious, since the linear model is also called RGB and, therefore, the conversion between color spaces must be done with some caution. It is also important to note that linear RGB values are rarely used to represent an image since they are perceptually highly non-uniform \citep{konstantinos:00}.


%% ------------------------------------------------------------------------- %%
\subsection{CMY color model}
\label{sec:modelo_cores_cmy}

The CMY model is based on the complementary primary colors Cyan, Magenta, and Yellow and, unlike RGB, is a subtractive color model in which colors are generated by subtracting the length of the dominant wave from the white light and, therefore, the resulting color corresponds to the light that is reflected \citep{gonzalez:02}.

One way to get the CMY system is:\\
\begin{equation}
  \begin{bmatrix}
    C \\ M \\ Y
  \end{bmatrix} = 
  \begin{bmatrix}
    B \\ R \\ R
  \end{bmatrix} +
  \begin{bmatrix}
    G \\ B \\ G
  \end{bmatrix}
\end{equation}

\noindent or by making a change of coordinates by subtracting the primary colors R, G and B of the white color $W = (1, 1, 1)$ \citep{gonzalez:02}:
\begin{equation}
  \begin{bmatrix}
    C \\ M \\ Y
  \end{bmatrix} = 
  \begin{bmatrix}
    1 \\ 1 \\ 1
  \end{bmatrix} -
  \begin{bmatrix}
    R \\ G \\ B
  \end{bmatrix}
\end{equation}

Likely RGB, CMY is device dependent. The model is widely used in equipment that deposits colored pigments on paper, such as color printers or photocopiers. The figure~\ref{fig:cmy-model} shows how the model components are combined to generate the other colors.

\begin{figure}[!h]
  \centering
  \includegraphics[width=.25\textwidth]{cmy-model}
  % https://en.wikipedia.org/wiki/File:SubtractiveColor.svg
  \caption[CMY subtractive color model]{CMY subtractive color model. It is interesting to note that the intersection of yellow with magenta generates the red color, magenta with cyan generates the blue color and cyan with yellow generates the green color. Source: \citet{rus:08}.}
  \label{fig:cmy-model}
\end{figure}

Overlapping the CMY primary colors in equal amounts to generate the black color typically creates a tint that is close to brown or dark green. To avoid this undesired effect, the black component is usually added to the system, represented by the letter K. This operation gives rise to a new model known as \textbf{CMYK}~\citep{gonzalez:02}.

%% ------------------------------------------------------------------------- %%
\subsection{Color models of the YUV family}
\label{sec:modelo_cores_yuv}

Color models of this family is also known as orthogonal color spaces. They are able to reduce the redundancy present in RGB color channels and represent the color with statistically independent components - as independent as possible \citep{kakumanu:07}.

The acronym YUV stands to a set of color spaces of which the luminance information, represented by the Y component, is coded separately from the chrominance, given by the components U and V. The components U and V are representations of signals of the difference of the blue subtracted from luminance (B-Y) and red subtracted from luminance (R-Y). It is used to represent colors in analogue television transmission systems in the Phase Alternating Line (PAL) and Sequential Color with Memory (SECAM)~\citep{pedrini:08}.

The transformation of the RGB space to YUV is given by:\\
\begin{equation}
  \begin{bmatrix}
    Y \\ U \\ V
  \end{bmatrix} = 
  \begin{bmatrix}
     0.299 &  0.587 &  0.114 \\
    -0.147 & -0.289 &  0.436 \\
     0.615 & -0.515 & -0.100 \\
  \end{bmatrix}
  \begin{bmatrix}
    R \\ G \\ B
  \end{bmatrix}
\end{equation}
where $0 \leq R, G, B \leq 1$.

Analogous to the YUV, the YIQ model was adopted in 1950 by the National Television System Committee (NTSC), an American standard for color television signal transmission. In this model, the Y component corresponds to luminance and the components I (hue) and Q (saturation) encode the chrominance information \citep{pedrini:08}.

The transformation of the RGB space to YIQ is given by:\\
\begin{equation}
  \begin{bmatrix}
    Y \\ I \\ Q
  \end{bmatrix} = 
  \begin{bmatrix}
    0.299  &  0.587 &  0.114 \\
    0.596  & -0.275 & -0.321 \\
    0.212  & -0.523 & -0.311 \\
  \end{bmatrix}
  \begin{bmatrix}
    R \\ G \\ B
  \end{bmatrix}
\end{equation}
where $0 \leq R, G, B \leq 1$.

Another color model of the YUV family is the YCbCr, mathematically defined by a coordinate transformation with respect to some RGB space~\citep{pedrini:08}.

The YCbCr model is widely used in digital videos. In this system, the Y component represents luminance, computed as a weighted sum of RGB values. Cb component gives the measurement of the difference between the blue color and a reference value, similar to the Cr component which is the measurement of the difference between the red color and a reference value~\citep{pedrini:08}.

The transformation of the RGB space to YCbCr is given by:\\
\begin{equation}
  \begin{bmatrix}
    Y \\ Cb \\ Cr
  \end{bmatrix} = 
  \begin{bmatrix}
     0.299 &  0.587 &  0.114 \\
    -0.169 & -0.331 &  0.5   \\
     0.5   & -0.419 & -0.081 \\
  \end{bmatrix}
  \begin{bmatrix}
    R \\ G \\ B
  \end{bmatrix}
\end{equation}


%% ------------------------------------------------------------------------- %%
\subsection{Color models of the HSI family}
\label{sec:modelo_cores_hsi}

Hue, Saturation, and Intensity (HSI) models are best suited for image processing applications from the user's point of view, due the correlation with human perception of the color\citep{konstantinos:00}.

In this model, as in YIQ, the intensity given by I component is decomposed from the chrominance information, represented by the hue (H) and saturation (S) \citep{konstantinos:00}. The combination of these components results in a pyramidal structure which can be seen in figure~\ref{fig:hsi-model}.

\begin{figure}[!ht]
  \centering
  \includegraphics[width=.7\textwidth]{hsi-model}
  % http://www.blackice.com/images/HSIColorModel.jpg
  \caption[Graphical representation of the HSI model]{Graphical representation of the HSI model. The hue describes the color itself, in the form of an angle $\theta$, where $\theta \in [0, 360]$. Red is at 0 degree, yellow at 60, green at 120, and so on. The saturation component, which varies between 0 and 1, indicates how much color is polluted with white color. The intensity scale is between $[0, 1]$, where 0 means black and 1, white. Source: \citet{blackice:16}.}
  \label{fig:hsi-model} 
\end{figure}

The transformation of the components of the RGB space to HSI is given by the equations:
\begin{align}
\label{eq:rgb_para_hsi}
\begin{split}
  \theta &= cos^{-1} \bigg( \frac{(R - G) + (R - B)}{2 \sqrt{(R - G)^2 + (R - B)(G - B)}} \bigg)
  \\[0.5em]
  H &= \begin{cases}
            \theta,       & \text{if}\ B \leq G\\
            360 - \theta, & \text{otherwise}\\
       \end{cases}
  \\[0.5em]
  S &= 1 - \frac{3 min(R, G, B)}{R + G + B}
  \\[0.5em]
  I &= \frac{R + G + B}{3}
\end{split}
\end{align}

It is important to note that the values R, G and B must be normalized in the interval between 0 and 1. The intensity $I$ and the saturation $S$ are also normalized between 0 and 1.

Another model of this family is formed by the components Hue, Saturation and Value (HSV) and its three-dimensional graphical representation is a hexagonal pyramid derived from the RGB cube \citep{pedrini:08}. Value, in this context, is the luminance component.

The various hue shades are represented at the top of the pyramid, the saturation is measured along the horizontal axis and value is measured along the vertical axis, which passes through the center of the pyramid. The hue, which corresponds to the edges around the vertical axis, varies from 0 (red) to 360 degrees and the angle between the vertices is 60 degrees. The saturation varies from 0 to 1 and is represented as the ratio of the purity of a given hue to its maximum purity, that is, when $S = 1$. Value varies from 0, at the peak of the pyramid representing the black color, to 1 at the base, where the intensities of the colors are maximum~\citep{pedrini:08}.

The transformation of the components of the RGB space to HSV is given by the equations:
\begin{align}
\label{eq:rgb_para_hsv}
\begin{split}
  H &=  \begin{cases}
            60\ffrac{(G - B)}{M - m}, & \text{if}\ M = R\\[0.7em]
            60\ffrac{(B - R)}{M - m} + 120, & \text{if}\ M = G\\[0.7em]
            60\ffrac{(R - G)}{M - m} + 240, & \text{if}\ M = B
       \end{cases}
  \\[0.5em]
  S &=  \begin{cases}
            \ffrac{(M - m)}{M}, \quad &\text{if}\ M \neq 0\\[0.7em]
            0, \quad &\text{otherwise}\\
       \end{cases}
  \\[0.5em]
  V &= M
\end{split}
\end{align}

\noindent where $m = min(R, G ,B)$ and $M = max(R, G ,B)$. The luminance $V$ and saturation $S$ are normalized between 0 and 1. The $H$ hue ranges from 0 to 360 degrees.

Similarly to HSV, the Hue, Saturation and Lightness (HSL) model is a three-dimensional representation and is formed by two cones of height 1, whose bases are coincident \citep{pedrini:08}.

The hue is determined by the points in the circle of the common bases to the cones. The saturation varies from 0 to 1, depending on the distance to the axis of the cone. The lightness is along the vertical axis common to the two cones and varies in the scale $[0, 1]$, where 0 means black and 1, white \citep{pedrini:08}.

The conversion of the RGB space to HSL is given by the equations:
\begin{align}
\label{eq:rgb_para_hsl}
\begin{split}
  H &=  \begin{cases}
            60\ffrac{(G - B)}{M - m}, & \text{if}\ M = R\\[0.7em]
            60\ffrac{(B - R)}{M - m} + 120, & \text{if}\ M = G\\[0.7em]
            60\ffrac{(R - G)}{M - m} + 240, & \text{if}\ M = B
       \end{cases}
  \\[0.5em]
  S &=  \begin{cases}
            \ffrac{(M - m)}{M + m}, & \text{if}\ 0 < L \leq 0,5\\[0.7em]
            \ffrac{(M - m)}{2 - (M + m)}, & \text{if}\ L > 0,5\\[0.5em]
            0, & \text{if}\ M = m\\
       \end{cases}
  \\[0.5em]
  L &= \frac{M + m}{2}
\end{split}
\end{align}

\noindent where $m = min(R, G ,B)$ and $M = max(R, G ,B)$. The lightness $V$ and saturation $S$ are normalized between 0 and 1. Note that the transformation of the $H$ component is the same as that used in the conversion of the RGB to HSV space in \ref{eq:rgb_para_hsv} and varies between 0 and 360 degrees.

All the color models of this family have the property of thinking of lighter colors, obtained by increasing the brightness or lightness, and darker colors, by the diminution of the same values. The intermediate colors are produced by decreasing the saturation~\citep{pedrini:08}.


%% ------------------------------------------------------------------------- %%
\section{Machine learning}
\label{sec:classificadores}
Machine learning is an area that is concerned with the development of computer programs capable of improving automatically with the experience \citep{mitchell:97}. This definition is closely linked to the way humans learn. Research efforts in this area have been carried out with the purpose of bringing this relationship closer.

As an example, in showing the image of a tree to a three-year-old child, she will most likely know how to recognize it, as she must have been exposed to situations where she has seen similar images, and was trained to do so answer. Then, she learned what a tree is by looking at them, not necessarily by precise mathematical definitions. In other words, the learning was done on the basis of data that are often used to obtain empirical solutions to certain problems where there is no possibility of creating an analytical solution \citep{mostafa:12}.

Similarly, an algorithm can be trained to differentiate a tree from other objects based on a set of data that has descriptions about the trees, such as height, colors, thickness, length, and so on. These descriptions are also called attributes, properties or features, and are submitted to a classifier that evaluates the presented evidences and makes a decision about the object being analyzed \citep{duda:12}. This task begins with the definition of a feature vector in a $d$-dimensional space, of the form \citep{duda:12}:

\begin{equation}
\label{eq:vetor_caracteristicas}
  x = 
  \begin{bmatrix}
    a_1 \\ a_2 \\ \vdots \\ a_d
  \end{bmatrix}
\end{equation}
\noindent where $x \in X$ and $X$ is the input space, that is, all of the $x$ possible vectors.

The dataset is formed by $N$ of these vectors and, therefore, the problem now is to partition the feature space in such a way that a decision boundary is formed \citep{duda:12}. We can then assign a specific class or label $y$ for a given vector, where $y \in Y$ and $Y$ is a finite set of classes which, in the binary case, is of the form $Y = \{+1, -1\}$. Figure~\ref{fig:decision_boundary} shows partitioning examples of a $2$-dimensional space of skin and non-skin samples from the SFA dataset \footnote{This dataset will be detailed in the section \ref{sec:datasets}.}.

\begin{figure}[ht]
    \centering
    \begin{minipage}{0.48\textwidth}
        \includegraphics[width=\textwidth]{decision_boundary_linear}
        \label{fig:decision_boundary_linear}
    \end{minipage}
    ~ % space
    \begin{minipage}{0.48\textwidth}
        \includegraphics[width=\textwidth]{decision_boundary_smooth}
        \label{fig:decision_boundary_smooth}
    \end{minipage}
    \caption[Decision boundary in the 2-dimensional space]{Decision boundary in the 2-dimensional space of skin and non-skin samples of the SFA dataset. The axes represent the channels $a$ and $b$ of the Lab color model. The figures on the left and right show the separation of the feature space by linear and non-linear functions, respectively. It is noteworthy that, computational complexity increases significantly in cases where there are many features. Source: proposed by the author.}
    \label{fig:decision_boundary}
\end{figure}

Note that the decision boundary is a hypothetical function that the learning algorithm must produce based on training data. It should be as close as possible to a target function or objective function $f: X \mapsto Y$ that is unknown \citep{mostafa:12}.

This type of approach is characterized as supervised learning, that is, the training data contain explicit samples of how the correct output should be on an input vector \citep{mostafa:12}.

In unsupervised learning, also known as clustering, it is unknown what classes the training data belong to. The mission of the classifier, in this case, is to agglomerate input data into natural clusters \citep{duda:12}. Therefore, unsupervised learning can be seen as a task of finding patterns or structures spontaneously from the input data \citep{mostafa:12}.

Another approach is reinforcement learning which, just as in unsupervised learning, does not use labeled input data. The output proposed by the training is used along with a measure of its quality to improve the results of the classifier \citep{mostafa:12}.

Still on the results of the classifier, it is important to point out that learning the parameters of a target function and testing it on the same data is a fundamental error because the model would repeat the labels of the samples as soon as they were trained, implying a perfect fit of the data. However, it would not be enough to predict anything useful about new input data. This phenomenon is called over-fitting and to avoid it, the dataset is then partitioned so as to store some of the data for a subsequent test step, in order to find the best-performing estimator \citep{mostafa:12}.

Splitting the data into disjoint training and testing subsets is an effective approach when a large amount of data is available. However, when data is limited, retaining part of them for the test set further reduces the number of samples available for training \citep{mitchell:97}. Therefore, another approach, known as cross-validation, can be applied so that the entire dataset is used in training and testing.

The cross-validation consists of dividing the dataset $D$ into $K$ disjoint subsets $D_1, D_2, \ldots, D_K$, where each subset has an approximate size of $N / K$. The model is then trained on each of these subsets, except one that is maintained as a validation set, in which the error measure is calculated. This process is repeated $K$ times, in such a way that each of the subsets has the opportunity to act as the test set. By this reason, this approach is also known as \emph{K-fold}. At the end of all $K$ iterations, the mean error obtained by each estimator is used as the performance measure of the classifier \citep{mostafa:12}.