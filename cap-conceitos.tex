%% ------------------------------------------------------------------------- %%
\chapter{Theoretical Background}
\label{cap:conceitos}
In this chapter, the theoretical concepts that apply to this research are stated. First, a short introduction to color models is provided in order to give an overview of the main characteristics of some of the most used in the computer vision and image processing area, on which this research is based. Thereafter, some machine learning methods are defined and explained, once they were used in the preliminary experiments of this work.                                                                                                                               

%% ------------------------------------------------------------------------- %%
\section{Color models}
\label{sec:fundamentos}

The use of color images in computer vision or image processing can be motivated by two main factors. The first refers to the powerful characteristic of color to function as a descriptor that often simplifies the identification and extraction of an object in a scene. The second is related to the ability of humans to discern thousands of tonalities and intensities compared to only a few dozen levels of gray \citep{gonzalez:02}.

The visual perception of color by the human eye should not vary according to the spectral distribution of the natural light incident upon an object. In other words, the color appearance of objects remains stable under different lighting conditions. This phenomenon is known as color constancy \citep{gevers:12}.

As an example, the grass of a soccer stadium remains green throughout the day, even at dusk when, from a physical point of view, sunlight has a more reddish appearance.

The human perception of colors occurs by the activation of nerve cells that send signals to the brain about brightness, hue and saturation, which are usually the features used to distinguish one color from another \citep{gonzalez:02}.

The brightness gives the notion of chromatic intensity. Hue represents the dominant color perceived by an observer. Saturation refers to the relative purity or amount of white light applied to the hue. Combined, hue and saturation are known as chromaticity and, therefore, a color must be characterized by its brightness and chromaticity \citep{gonzalez:02}.

Colors can be specified by mathematical models in tuples of numbers in a coordinate system and a subspace within that system where each color is represented by a single point. Such models are known as the color models \citep{gonzalez:02}.

These models can be classified as of two types: the additive models in which the primary color intensities are added to produce other colors and subtractive, where colors are generated by subtracting the length of the dominant wave from the white light.

The following sections briefly describe some of the major color models, as well as their variants and main areas of application.

%% ------------------------------------------------------------------------- %%
\subsection{Munsell color model}
\label{sec:modelo_cores_munsell}

Pioneer in an attempt to organize the perception of color in a color space, Albert H. Munsell was able to combine the art and science of colors in a single theory \citep{konstantinos:00}.

The principle of equality of visual spacing between the components of the model is the essential idea of the Munsell color model. These components are hue, value, corresponding to luminance, and chroma, corresponding to saturation \citep{konstantinos:00}.

The model is represented by a cylindrical shape and it can be seen in the figure ~\ref{fig:munsell-system}. The hue is arranged in the circular axis consisting of five base as well as five secondary colors, the saturation in the radial axis and the luminance in the vertical axis in a range varying from 0 to 10.

\begin{figure}[!h]
  \centering
  \includegraphics[width=.55\textwidth]{munsell-system}
  % fonte https://commons.wikimedia.org/wiki/File:Munsell-system.svg
  \caption[Munsell color model.]{Munsell color model represented by a cylindrical shape. The hue is arranged on the circular axis consisting of five base and five secondary colors, the saturation on the radial axis and the luminance on the vertical axis in a range varying from 0 to 10. Source: \citet{rus:07}.}
  \label{fig:munsell-system} 
\end{figure}

%% ------------------------------------------------------------------------- %%
\subsection{CIE color model}
\label{sec:modelo_cores_cie}

In 1931, the CIE established the first mathematical model of color numerical specification, whose objective was to analyze the relationship between the physical aspects of colors in the electromagnetic spectrum and their perception by the human visual system to determine how an ordinary person perceives the color. A review of this specification was published in 1964 \citep{gonzalez:02}.

The experiment that originated the standard consisted in detecting the colors perceived by an observer from a mixture of three primary colors X, Y and Z called tristimulus values. These coordinates gave rise to the CIE XYZ color space which encompasses all the colors that can be perceived by an ordinary human being. For this reason, it is considered an device independent representation \citep{konstantinos:00}.

The system proposed by the CIE XYZ to describe a color is based on a luminance component Y, and two additional components X and Z, that bring the chromaticity information. This system is formed by imaginary colors that can be expressed as combinations of the normalized measures shown in the equations~\ref{eq:cie_x}, \ref{eq:cie_y} and \ref{eq:cie_z}.

\begin{equation}
  x = \frac{X}{X + Y + Z}
\label{eq:cie_x}
\end{equation}

\begin{equation}
  y = \frac{Y}{X + Y + Z}
\label{eq:cie_y}
\end{equation}

\begin{equation}
  z = \frac{Z}{X + Y + Z}
\label{eq:cie_z}
\end{equation}

where $x + y+ z = 1$.

Combinations of negative values and other problems related to selecting a set of real primaries are eliminated. The chromaticity coordinates $x$ and $y$ allow to represent all colors in a two-dimensional plane, also known as a chromaticity diagram, which can be seen in the figure ~\ref{fig:cie-cromaticity-diagram}.

\begin{figure}[!h]
  \centering
  \includegraphics[width=.5\textwidth]{cie-cromaticity-diagram}
  % fonte https://en.wikipedia.org/wiki/File:CIE1931xy_blank.svg
  \caption[CIE 1931 chromaticity diagram]{CIE 1931 chromaticity diagram. The points representing pure colors in the electromagnetic spectrum are labeled according to their wavelengths and are located along the curve from the right end of the x-axis, corresponding to the red color, to the left end of the same axis, corresponding to the violet color, forming a polygon similar to a horseshoe. The internal points correspond to all possible combinations of visible colors. Source: \citet{ben:09}.}
  \label{fig:cie-cromaticity-diagram} 
\end{figure}

The coordinates $ (x = 1/3, y = 1/3) $ correspond to the location of white light, also known as white point, and serve as reference in the process of image capture, coding, or reproduction.

CIE also derived and standardized two other color models based on CIE XYZ specification and, likewise, are device independent. Both are perceptually uniform, which means that equal perceptual distances separate all colors in the system~\citep{vezhnevets:03}. As an example, the gray scale of the space should allow for a smooth transition between black and white.

The first one was designed to reduce the problem of perceptual non-uniformity. Some Uniform Chromaticity Scale (UCS) diagrams were proposed based on mathematical equations to transform the values XYZ or the coordinates $x, y$ into a new set of values $(u, v)$, which gave rise to the 1960 CIE $uv$ chromaticity diagram~\citep{gevers:12}.

Still with unsatisfactory results, the CIE made a new change by multiplying the $v$ component by a factor of 1.5. In addition, the brightness scale given by the Y component has been replaced by $L^* = [0, 100]$ to better represent the differences in luminosity that are equivalent. This revision originated the CIE 1976 $L^*u^*v^*$ color model, commonly known by the acronym CIELuv~\citep{gevers:12}.

In 1976 the CIE adopted a new color model, based on the $L, a, b$ model, proposed by Richard Hunter in 1948, which best represented the uniform spacing of colors. Named CIE $L^*a^*b^*$ and known by the acronym CIELab, it is a space based on opponent colors \footnote{Theory started around 1500 when Leonardo da Vinci concluded that colors are produced by mixing yellow and blue, green and red, and white and black. In 1950, this theory was confirmed when optically-colored signals were detected at the optical connection between the retina and the brain~\citep{gevers:12}.} in which the color stimuli of retina is converted to distinctions between light and dark, red and green, and blue and yellow, represented by the axes $L^*$, $a^*$, and $b^*$, respectively~\citep{gevers:12}.


%% ------------------------------------------------------------------------- %%
\subsection{RGB color model}
\label{sec:modelo_cores_rgb}

The RGB model, an acronym for Red, Green, and Blue, is an additive color model in which the three primary colors red, green and blue are added to produce the others \citep{gonzalez:02}.

This system was based on the trichromatic theory of Thomas Young and Hermann Helmholtz in the mid-19th century and can be represented graphically
through the unit cube defined on the axes R, G and B, as illustrated in the figure~\ref{fig:rgb-cube} \citep{konstantinos:00}.

\begin{figure}[!h]
  \centering
  \includegraphics[width=.35\textwidth]{rgb-cube}
  % http://www.scratchapixel.com/old/lessons/3d-basic-lessons/lesson-5-colors-and-digital-images/color-spaces/
  \caption[Unit cube representing the colors of the RGB model]{Unit cube representing the colors of the RGB model. The origin, given by the vertex $(0, 0, 0)$, represents the black color. The vertex $(1, 1, 1)$, opposite the origin, represents the white color. The highlighted vertices on the axes represent the primary colors and the others are the complement of each. Each point inside the cube corresponds to a color that can be represented by the triple $(r, g, b)$, where $r, g, b \in [0, 1]$. The shades of gray are represented along the main diagonal of the cube, with each point along this diagonal being formed by equal contributions of each primary color. Source: adapted from \citet{gonzalez:02}.}
  \label{fig:rgb-cube} 
\end{figure}

It is noteworthy that there are two ways of representing the RGB space: linear and non-linear. The above-mentioned system shows the non-linear model, whose abbreviation is $R'G'B'$, and is most used by devices and applications because of their similarity to the human visual system. In the literature, this system is frequently cited with the acronym RGB, which makes the nomenclature dubious, since the linear model is also called RGB and, therefore, the conversion between color spaces must be done with some caution. It is also important to note that linear RGB values are rarely used to represent an image since they are perceptually highly non-uniform \citep{konstantinos:00}.


%% ------------------------------------------------------------------------- %%
\subsection{CMY color model}
\label{sec:modelo_cores_cmy}

The CMY model is based on the complementary primary colors Cyan, Magenta, and Yellow and, unlike RGB, is a subtractive color model in which colors are generated by subtracting the length of the dominant wave from the white light and, therefore, the resulting color corresponds to the light that is reflected \citep{gonzalez:02}.

One way to get the CMY system is:\\
\begin{equation}
  \begin{bmatrix}
    C \\ M \\ Y
  \end{bmatrix} = 
  \begin{bmatrix}
    B \\ R \\ R
  \end{bmatrix} +
  \begin{bmatrix}
    G \\ B \\ G
  \end{bmatrix}
\end{equation}

\noindent or by making a change of coordinates by subtracting the primary colors R, G and B of the white color $W = (1, 1, 1)$ \citep{gonzalez:02}:
\begin{equation}
  \begin{bmatrix}
    C \\ M \\ Y
  \end{bmatrix} = 
  \begin{bmatrix}
    1 \\ 1 \\ 1
  \end{bmatrix} -
  \begin{bmatrix}
    R \\ G \\ B
  \end{bmatrix}
\end{equation}

Likely RGB, CMY is device dependent. The model is widely used in equipment that deposits colored pigments on paper, such as color printers or photocopiers. The figure~\ref{fig:cmy-model} shows how the model components are combined to generate the other colors.

\begin{figure}[!h]
  \centering
  \includegraphics[width=.25\textwidth]{cmy-model}
  % https://en.wikipedia.org/wiki/File:SubtractiveColor.svg
  \caption[CMY subtractive color model]{CMY subtractive color model. It is interesting to note that the intersection of yellow with magenta generates the red color, magenta with cyan generates the blue color and cyan with yellow generates the green color. Source: \citet{rus:08}.}
  \label{fig:cmy-model}
\end{figure}

Overlapping the CMY primary colors in equal amounts to generate the black color typically creates a tint that is close to brown or dark green. To avoid this undesired effect, the black component is usually added to the system, represented by the letter K. This operation gives rise to a new model known as \textbf{CMYK}~\citep{gonzalez:02}.

%% ------------------------------------------------------------------------- %%
\subsection{Color models of the YUV family}
\label{sec:modelo_cores_yuv}

The acronym YUV stands to a family of color spaces of which the luminance information, represented by the Y component, is coded separately from the chrominance, given by the components U and V. The components U and V are representations of signals of the difference of the blue subtracted from luminance (B-Y) and red subtracted from luminance (R-Y). It is used to represent colors in analogue television transmission systems in the Phase Alternating Line (PAL) and Sequential Color with Memory (SECAM)~\citep{pedrini:08}.

The transformation of the RGB space to YUV is given by:\\
\begin{equation}
  \begin{bmatrix}
    Y \\ U \\ V
  \end{bmatrix} = 
  \begin{bmatrix}
     0.299 &  0.587 &  0.114 \\
    -0.147 & -0.289 &  0.436 \\
     0.615 & -0.515 & -0.100 \\
  \end{bmatrix}
  \begin{bmatrix}
    R \\ G \\ B
  \end{bmatrix}
\end{equation}
where $0 \leq R, G, B \leq 1$.

Analogous to the YUV, the YIQ model was adopted in 1950 by the National Television System Committee (NTSC), an American standard for color television signal transmission. In this model, the Y component corresponds to luminance and the components I (hue) and Q (saturation) encode the chrominance information \citep{pedrini:08}.

The transformation of the RGB space to YIQ is given by:\\
\begin{equation}
  \begin{bmatrix}
    Y \\ I \\ Q
  \end{bmatrix} = 
  \begin{bmatrix}
    0.299  &  0.587 &  0.114 \\
    0.596  & -0.275 & -0.321 \\
    0.212  & -0.523 & -0.311 \\
  \end{bmatrix}
  \begin{bmatrix}
    R \\ G \\ B
  \end{bmatrix}
\end{equation}
where $0 \leq R, G, B \leq 1$.

Another color model of the YUV family is the YCbCr, mathematically defined by a coordinate transformation with respect to some RGB space~\citep{pedrini:08}.

The YCbCr model is widely used in digital videos. In this system, the Y component represents luminance, Cb component gives the measurement of the difference between the blue color and a reference value, similar to the Cr component which is the measurement of the difference between the red color and a reference value~\citep{pedrini:08}.

The transformation of the RGB space to YCbCr is given by:\\
\begin{equation}
  \begin{bmatrix}
    Y \\ Cb \\ Cr
  \end{bmatrix} = 
  \begin{bmatrix}
     0.299 &  0.587 &  0.114 \\
    -0.169 & -0.331 &  0.5   \\
     0.5   & -0.419 & -0.081 \\
  \end{bmatrix}
  \begin{bmatrix}
    R \\ G \\ B
  \end{bmatrix}
\end{equation}


%% ------------------------------------------------------------------------- %%
\subsection{Color models of the HSI family}
\label{sec:modelo_cores_hsi}

Hue, Saturation, and Intensity (HSI) models are best suited for image processing applications from the user's point of view, due the correlation with human perception of the color~\citep{konstantinos:00}.

In this model, as in YIQ, the intensity given by I component is decomposed from the chrominance information, represented by the hue (H) and saturation (S) \citep{konstantinos:00}. The combination of these components results in a pyramidal structure which can be seen in figure~\ref{fig:hsi-model}.

\begin{figure}[!h]
  \centering
  \includegraphics[width=.7\textwidth]{hsi-model}
  % http://www.blackice.com/images/HSIColorModel.jpg
  \caption[Graphical representation of the HSI model]{Graphical representation of the HSI model. The hue describes the color itself, in the form of an angle $\theta$, where $\theta \in [0, 360]$. Red is at 0 degree, yellow at 60, green at 120, and so on. The saturation component, which varies between 0 and 1, indicates how much color is polluted with white color. The intensity scale is between $[0, 1]$, where 0 means black and 1, white. Source: \citet{blackice:16}.}
  \label{fig:hsi-model} 
\end{figure}

The transformation of the components of the RGB space to HSI is given by the equations:
\begin{align}
\label{eq:rgb_para_hsi}
\begin{split}
  \theta &= cos^{-1} \bigg( \frac{(R - G) + (R - B)}{2 \sqrt{(R - G)^2 + (R - B)(G - B)}} \bigg)
  \\[0.5em]
  H &= \begin{cases}
            \theta,       & \text{if}\ B \leq G\\
            360 - \theta, & \text{otherwise}\\
       \end{cases}
  \\[0.5em]
  S &= 1 - \frac{3 min(R, G, B)}{R + G + B}
  \\[0.5em]
  I &= \frac{R + G + B}{3}
\end{split}
\end{align}

It is important to note that the values R, G and B must be normalized in the interval between 0 and 1. The intensity $I$ and the saturation $S$ are also normalized between 0 and 1.

Another model of this family is formed by the components Hue, Saturation and Value (HSV) and its three-dimensional graphical representation is a hexagonal pyramid derived from the RGB cube \citep{pedrini:08}. Value, in this context, is the luminance component.

The various hue shades are represented at the top of the pyramid, the saturation is measured along the horizontal axis and value is measured along the vertical axis, which passes through the center of the pyramid. The hue, which corresponds to the edges around the vertical axis, varies from 0 (red) to 360 degrees and the angle between the vertices is 60 degrees. The saturation varies from 0 to 1 and is represented as the ratio of the purity of a given hue to its maximum purity, that is, when $S = 1$. Value varies from 0, at the peak of the pyramid representing the black color, to 1 at the base, where the intensities of the colors are maximum~\citep{pedrini:08}.

The transformation of the components of the RGB space to HSV is given by the equations:
\begin{align}
\label{eq:rgb_para_hsv}
\begin{split}
  H &=  \begin{cases}
            60\ffrac{(G - B)}{M - m}, & \text{if}\ M = R\\[0.7em]
            60\ffrac{(B - R)}{M - m} + 120, & \text{if}\ M = G\\[0.7em]
            60\ffrac{(R - G)}{M - m} + 240, & \text{if}\ M = B
       \end{cases}
  \\[0.5em]
  S &=  \begin{cases}
            \ffrac{(M - m)}{M}, \quad &\text{if}\ M \neq 0\\[0.7em]
            0, \quad &\text{otherwise}\\
       \end{cases}
  \\[0.5em]
  V &= M
\end{split}
\end{align}

\noindent where $m = min(R, G ,B)$ and $M = max(R, G ,B)$. The luminance $V$ and saturation $S$ are normalized between 0 and 1. The $H$ hue ranges from 0 to 360 degrees.

Similarly to HSV, the Hue, Saturation and Lightness (HSL) model is a three-dimensional representation and is formed by two cones of height 1, whose bases are coincident \citep{pedrini:08}.

The hue is determined by the points in the circle of the common bases to the cones. The saturation varies from 0 to 1, depending on the distance to the axis of the cone. The lightness is along the vertical axis common to the two cones and varies in the scale $[0, 1]$, where 0 means black and 1, white \citep{pedrini:08}.

The conversion of the RGB space to HSL is given by the equations:
\begin{align}
\label{eq:rgb_para_hsl}
\begin{split}
  H &=  \begin{cases}
            60\ffrac{(G - B)}{M - m}, & \text{if}\ M = R\\[0.7em]
            60\ffrac{(B - R)}{M - m} + 120, & \text{if}\ M = G\\[0.7em]
            60\ffrac{(R - G)}{M - m} + 240, & \text{if}\ M = B
       \end{cases}
  \\[0.5em]
  S &=  \begin{cases}
            \ffrac{(M - m)}{M + m}, & \text{if}\ 0 < L \leq 0,5\\[0.7em]
            \ffrac{(M - m)}{2 - (M + m)}, & \text{if}\ L > 0,5\\[0.5em]
            0, & \text{if}\ M = m\\
       \end{cases}
  \\[0.5em]
  L &= \frac{M + m}{2}
\end{split}
\end{align}

\noindent where $m = min(R, G ,B)$ and $M = max(R, G ,B)$. The lightness $V$ and saturation $S$ are normalized between 0 and 1. Note that the transformation of the $H$ component is the same as that used in the conversion of the RGB to HSV space in \ref{eq:rgb_para_hsv} and varies between 0 and 360 degrees.

All the color models of this family have the property of thinking of lighter colors, obtained by increasing the brightness or lightness, and darker colors, by the diminution of the same values. The intermediate colors are produced by decreasing the saturation~\citep{pedrini:08}.


%% ------------------------------------------------------------------------- %%
\section{Machine learning}
\label{sec:classificadores}
Machine learning is an area that is concerned with the development of computer programs capable of improving automatically with the experience \citep{mitchell:97}. This definition is closely linked to the way humans learn. Research efforts in this area have been carried out with the purpose of bringing this relationship closer.

As an example, in showing the image of a tree to a three-year-old child, she will most likely know how to recognize it, as she must have been exposed to situations where she has seen similar images, and was trained to do so answer. Then, she learned what a tree is by looking at them, not necessarily by precise mathematical definitions. In other words, the learning was done on the basis of data that are often used to obtain empirical solutions to certain problems where there is no possibility of creating an analytical solution \citep{mostafa:12}.

Similarly, an algorithm can be trained to differentiate a tree from other objects based on a set of data that has descriptions about the trees, such as height, colors, thickness, length, and so on. These descriptions are also called attributes, properties or features, and are submitted to a classifier that evaluates the presented evidences and makes a decision about the object being analyzed \citep{duda:12}. This task begins with the definition of a feature vector in a $d$-dimensional space, of the form \citep{duda:12}:

\begin{equation}
\label{eq:vetor_caracteristicas}
  x = 
  \begin{bmatrix}
    a_1 \\ a_2 \\ \vdots \\ a_d
  \end{bmatrix}
\end{equation}
\noindent where $x \in X$ and $X$ is the input space, that is, all of the $x$ possible vectors.

The dataset is formed by $N$ of these vectors and, therefore, the problem now is to partition the feature space in such a way that a decision boundary is formed \citep{duda:12}. We can then assign a specific class or label $y$ for a given vector, where $y \in Y$ and $Y$ is a finite set of classes which, in the binary case, is of the form $Y = \{+1, -1\}$. Figure~\ref{fig:decision_boundary} shows partitioning examples of a $2$-dimensional space of skin and non-skin samples from the SFA dataset \footnote{This dataset will be detailed in the section \ref{sec:datasets}.}.

\begin{figure}[ht]
    \centering
    \begin{minipage}{0.48\textwidth}
        \includegraphics[width=\textwidth]{decision_boundary_linear}
        \label{fig:decision_boundary_linear}
    \end{minipage}
    ~ % space
    \begin{minipage}{0.48\textwidth}
        \includegraphics[width=\textwidth]{decision_boundary_smooth}
        \label{fig:decision_boundary_smooth}
    \end{minipage}
    \caption[Decision boundary in the 2-dimensional space]{Decision boundary in the 2-dimensional space of skin and non-skin samples of the SFA dataset. The axes represent the channels $a$ and $b$ of the Lab color model. The figures on the left and right show the separation of the feature space by linear and non-linear functions, respectively. It is noteworthy that, computational complexity increases significantly in cases where there are many features. Source: proposed by the author.}
    \label{fig:decision_boundary}
\end{figure}

Note that the decision boundary is a hypothetical function that the learning algorithm must produce based on training data. It should be as close as possible to a target function or objective function $f: X \mapsto Y$ that is unknown \citep{mostafa:12}.

This type of approach is characterized as supervised learning, that is, the training data contain explicit samples of how the correct output should be on an input vector \citep{mostafa:12}.

In unsupervised learning, also known as clustering, it is unknown what classes the training data belong to. The mission of the classifier, in this case, is to agglomerate input data into natural clusters \citep{duda:12}. Therefore, unsupervised learning can be seen as a task of finding patterns or structures spontaneously from the input data \citep{mostafa:12}.

Another approach is reinforcement learning which, just as in unsupervised learning, does not use labeled input data. The output proposed by the training is used along with a measure of its quality to improve the results of the classifier \citep{mostafa:12}.

Still on the results of the classifier, it is important to point out that learning the parameters of a target function and testing it on the same data is a fundamental error because the model would repeat the labels of the samples as soon as they were trained, implying a perfect fit of the data. However, it would not be enough to predict anything useful about new input data. This phenomenon is called over-fitting and to avoid it, the dataset is then partitioned so as to store some of the data for a subsequent test step, in order to find the best-performing estimator \citep{mostafa:12}.

Splitting the data into disjoint training and testing subsets is an effective approach when a large amount of data is available. However, when data is limited, retaining part of them for the test set further reduces the number of samples available for training \citep{mitchell:97}. Therefore, another approach, known as cross-validation, can be applied so that the entire dataset is used in training and testing.

The cross-validation consists of dividing the dataset $D$ into $K$ disjoint subsets $D_1, D_2, \ldots, D_K$, where each subset has an approximate size of $N / K$. The model is then trained on each of these subsets, except one that is maintained as a validation set, in which the error measure is calculated. This process is repeated $K$ times, in such a way that each of the subsets has the opportunity to act as the test set. By this reason, this approach is also known as \emph{K-fold}. At the end of all $K$ iterations, the mean error obtained by each estimator is used as the performance measure of the classifier \citep{mostafa:12}.

Some of the classifiers that were used in the preliminary experiments of the chapter \ref{cap:experimentos} will be briefly discussed in the sections \ref{sec:classifiers_svm}, \ref{sec:classifiers_knn} and \ref{sec:classifiers_dt}.



%% ------------------------------------------------------------------------- %%
\subsection{Support vector machines}
\label{sec:classifiers_svm}
Support Vector Machines (SVM) are a machine learning technique based on the Statistical Learning Theory \citep{vapnik:13}, whose objective was to solve problems of classification of patterns. In practice, an SVM has the ability to generate a hyperplane or set of hyperplanes in a space of high or infinite dimensionality, which can be used for classification, regression, or other tasks \citep{duda:12}.

Derived from the technique name itself, the support vectors are the training set samples that define the optimal hyperplane and are the most difficult patterns to classify \citep{duda:12}. Intuitively, they are the most relevant patterns for the classification task, since a change in these vectors directly implies in the result of the optimal hyperplane \footnote{The optimal separation hyperplane of an SVM is that of a class of hyperplanes with the highest separation margin between the two training sets \citep{cortes:95}.}.

Let the training dataset with $N$ samples of the form:
\begin{equation}
\label{eq:svm_dataset}
    D = (x_1, y_1), (x_2, y_2), \ldots, (x_N, y_N)
\end{equation}
\noindent where each $x_i$ is a $d$-dimensional vector of the form given by \ref{eq:vetor_caracteristicas}, $i = 1, 2, \ldots, N$, $y_i \in Y$ and $Y = \{+1, -1\}$. Therefore, the training dataset contain $N$ observations with its respective labels.

Supposing that $D$ is linearly separable, one can separate the data by means of a hyperplane using a classifier, also linear, defined by the equation \citep{lorena:03}:
\begin{equation}
\label{eq:svm_hyperplano_otimo}
w \cdot x + b = 0
\end{equation}
\noindent where $w \cdot x$ is a dot product, $w$ is the normal vector to the hyperplane and $b$ is a bias term. The parameter $\ffrac{b}{||w||}$ determines the offset of the hyperplane in relation to the origin.

From this definition, two other parallel hyperplanes to the optimal hyperplane can be obtained, according to the equations in~\ref{eq:svm_hyperplanos_paralelos}, so that a delimited region, known as margin, rises between them.

\begin{equation}
\label{eq:svm_hyperplanos_paralelos}
\begin{cases}
    w \cdot x + b = +1\\
    w \cdot x + b = -1
\end{cases}
\end{equation}

In addition, some constraints are defined to avoid that there are no points between $w \cdot x + b = 0 $ and $w \cdot x + b = \pm 1$. Formally, we have \citep{lorena:03}:

\begin{equation}
\label{eq:svm_hyperplanos_paralelos_restricoes}
\begin{cases}
    w \cdot x_i + b \geq +1, & \text{if}\ y_i = +1\\
    w \cdot x_i + b \leq -1, & \text{if}\ y_i = -1
\end{cases}
\end{equation}
\noindent or, similarly:
\begin{equation}
\label{eq:svm_hyperplanos_paralelos_restricoes_equiv}
    y_i (w \cdot x_i + b) \geq 1
\end{equation}

According to \citet{campbell:00}, in the system given by equation \ref{eq:svm_hyperplanos_paralelos_restricoes}, it is assumed that the margin is always greater than the distance between $w \cdot x + b = 0$ and $|w \cdot x + b = 1|$ and, for this reason, SVMs of this nature are usually referred as hard-margin SVMs. Figure~\ref{fig:svm_support_vectors} shows the graphical representation of these concepts.

\begin{figure}[ht]
  \centering
  \includegraphics[width=.5\textwidth]{svm_support_vectors}
  \caption[Example of a problem of separable binary classes in a $2$-dimensional space]{Example of a problem of separable binary classes in a $2$-dimensional space. The support vectors, marked with gray squares, define the margin with highest separation between the two classes. Source: \citet{cortes:95}.}
  \label{fig:svm_support_vectors}
\end{figure}

Geometrically, the distance between the two parallel hyperplanes to the optimal hyperplane is $\ffrac{2}{||w||}$. Consequently, one can deduce that the distance between the optimal hyperplane and $w \cdot x + b = \pm 1$ is $\ffrac{1}{||w||}$. Then, the minimization of $||w||$ maximizes the margin and, thus, we have an optimization problem in which we want to minimize $||w||^2$ subject to the constraints in \ref{eq:svm_hyperplanos_paralelos_restricoes_equiv} \citep{lorena:03}. Therefore, optimal $w$ and $b$ that solve this problem define the classifier and can be obtained by Lagrange multipliers \citep{campbell:00}:

\begin{equation}
\label{eq:svm_margens_rigidas}
\text{Maximize: } \sum_{i=1}^N \alpha_i - \frac{1}{2} \sum_{i, j=1}^N \alpha_i \alpha_j y_i y_j x_i\cdot x_j
\end{equation}

\begin{equation}
\label{eq:svm_margens_rigidas_restricoes}
\text{Subject to: }
\begin{cases}
    \alpha_i \geq 0\\[1em]
    \sum_{i=1}^N \alpha_i y_i = 0
\end{cases}
\end{equation}
\noindent where $\alpha$ are the Lagrange multipliers.

It is important to note that this type of SVM succeeds in linearly separable training datasets. In cases where the data are non-linearly separable, some classification errors are allowed for the training set, by the inclusion of relaxation variables \citep{lorena:03}. These changes were produced by \citet{cortes:95} and are part of a technique called margin smoothing, and therefore, SVMs of this family are also called soft-margin SVMs. So, the optimization problem becomes \citep{lorena:03}:
\begin{equation}
\label{eq:svm_margens_suaves_def}
\text{Minimize: } ||w||^2 + C\sum_{i=1}^N \xi_i
\end{equation}

\begin{equation}
\label{eq:svm_margens_suaves_def_restricoes}
\text{Subject to: }
\begin{cases}
    \xi_i \geq 0\\[1em]
    y_i (w \cdot x_i + b) \geq 1 - \xi_i
\end{cases}
\end{equation}
\noindent where $C$ is an regularization constant, empirically determined, that imposes a different weight for training in relation to generalization. Thus, the problem can be solved in the same way with the equation in \ref{eq:svm_margens_rigidas}, but with some changes in the constraints \citep{lorena:03}:

\begin{equation}
\label{eq:svm_margens_suaves_restricoes}
\text{Subject to: }
\begin{cases}
    0 \leq \alpha_i \leq C\\[1em]
    \sum_{i=1}^N \alpha_i y_i = 0
\end{cases}
\end{equation}

In general, most classification problems prevent linear classifiers from being used because it is not possible to obtain satisfactory results in the partitioning of training data by a hyperplane. However, linear SVMs can be extended to deal with this situation by mapping the input space into a higher-dimensional feature space, typically much larger than the original space~\citep{duda:12}, of the form:

\begin{equation}
\label{eq:svm_dataset_transformado}
    D' = (\Phi(x_1), y_1), (\Phi(x_2), y_2), \ldots, (\Phi(x_N), y_N)
\end{equation}

The proper choice of a function $\Phi$ makes the training set linearly separable in the transformed space \citep{lorena:03}. The shape of the optimal hyperplane is now defined by:
\begin{equation}
\label{eq:svm_hyperplano_otimo_trasnformado}
w \cdot \Phi(x) + b = 0
\end{equation}

Therefore, the concepts of support vectors, margin, parallel hyperplanes, and hence, optimization problem to find the solution for the $w$ and $b$ parameters apply here in a similar manner. Formally, the optimization problem can be solved as~\citep{lorena:03}:
\begin{equation}
\label{eq:svm_nao_linear}
\text{Maximize: } \sum_{i=1}^N \alpha_i - \frac{1}{2} \sum_{i, j=1}^N \alpha_i \alpha_j y_i y_j \Phi(x_i)\cdot \Phi(x_j)
\end{equation}
\noindent subject to the same constrains given by~\ref{eq:svm_margens_suaves_restricoes}.

Now, there is a need to define as the inner product $\Phi(x_i) \cdot \Phi(x_j)$ between two vectors $x_i, x_j \in D$ is realized, whose answer lies in the introduction of the concept of \emph{kernels}~\citep{lorena:03}.

\emph{Kernels} are functions that have the purpose of projecting the input vectors in a features space with exponential or infinite number of dimensions~\citep{taylor:04}, of the form:
\begin{equation}
\label{eq:svm_kernels}
k(x_i, x_j) =  \Phi(x_i) \cdot \Phi(x_j)
\end{equation}

Thus, it is possible to apply a \emph{kernel} as described in the equation~\ref{eq:svm_kernels} in the SVM optimization step, efficiently calculating the internal product from the input data, without even explicitly computing the mapping of the $\Phi$ function~\citep{taylor:04}.

Some of the most commonly used \emph{kernels} are~\citep{lorena:03}:
\begin{enumerate}[label=(\roman*)]
\item Linear \emph{kernel}
\begin{equation}
\label{eq:svm_kernel_linear}
k(x_i, x_j) =  (x_i \cdot x_j)
\end{equation}
The linear \emph{kernel} is the simplest of the \emph{kernel} functions and it is given by the internal product of two vectors $x_i$ and $x_j$.

\item Polynomial \emph{kernel}
\begin{equation}
\label{eq:svm_kernel_polinomial}
k(x_i, x_j) =  ({\gamma x_i \cdot x_j + r} )^g
\end{equation}
Where $g$ is the degree of the polynomial and $r$ is a constant term. For this \emph{kernel}, the $\Phi$ mappings are also polynomial functions, so the complexity is proportional to the choice of $g$~\citep{lorena:03}.

\item Gaussian \emph{kernel}
\begin{equation}
\label{eq:svm_kernel_rbf}
k(x_i, x_j) =  \exp{\big(-\gamma {||x_i - x_j||}^2 \big)}
\end{equation}
Also known as Radial Basis Function (RBF), the Gaussian \emph{kernel} corresponds to a features space of infinite dimension~\citep{lorena:03}.

\end{enumerate}

For the RBF and polynomial \emph{kernels}, the $\gamma$ parameter can be seen as the inverse of the radius of influence of samples selected by the model as support vectors \citep{scikit-learn:11}.



%% ------------------------------------------------------------------------- %%
\subsection{\emph{k}-Nearest Neighbors}
\label{sec:classifiers_knn}
The $k$-Nearest Neighbors ($k$-NN) is an algorithm based on instances, which means that the target function is not learned based on training samples; they are simply stored, so that the relation between a new sample and the stored training samples is evaluated, and a target function is then assigned to it~\citep{mitchell:97}.

As the name suggests, $k$-NN classifies a new point $x$ by assigning it the highest frequency class of the $k$ closest samples, that is, the decision of the class of $x$ is made by majority of the votes of the $k$ nearest neighbors, and for this reason, it is interesting that the choice of $k$ be an odd number to avoid draws \citep{duda:12}.

\begin{figure}[ht]
  \centering
  \includegraphics[width=.65\textwidth]{knn_exemplo}
  \caption[Example of a $k$-NN application in the classification task in a $2$-dimensional space]{Example of a $k$-NN application in the classification task in a $2$-dimensional space. The algorithm evaluates the $k$ samples near $x$, creating a spherical region, and labels $x$ with the highest frequency class. In this case, $k = 5$ and the class assigned to $x$ must be the same as the blue points. Source: proposed by the author.}
  \label{fig:knn_exemplo}
\end{figure}

Let $D$ be the training dataset with $N$ samples as given in \ref{eq:svm_dataset}. Therefore, the distance between two samples $x_i$ and $x_j$ can be obtained in terms of the Euclidean distance, defined as:

\begin{equation}
\label{eq:knn_distance_euclidian}
d(x_i, x_j) = \sqrt{\sum_{r=1}^d (a_r(x_i) - a_r(x_j))^2}
\end{equation}
\noindent where $a_r$ refers to the $r$th attribute of the input vector $x$ and $i, j = 1, 2, \ldots, N$. Other distance metrics can be attributed to $d(x_i, x_j)$, such as, Manhattan, Chebyshev, and Minkowski~\citep{duda:12}.

In order to classify a new sample $x_q$, we take $x_1, \ldots, x_k$ instances of the training set, whose distances given by \ref{eq:knn_distance_euclidian}, are of the $k$ points nearest to $x_q$. Thus, the function that estimates the class of $x_q$ is given by \citep{mitchell:97}:

\begin{equation}
\label{eq:knn_funcao_argmax}
g(x_q) \gets \argmax_{y \in Y} \sum_{i=1}^k \delta (y, f(x_i))
\end{equation}
\noindent where
\begin{equation}
\label{eq:knn_delta}
  \delta (y, f(x_i)) =  \begin{cases}
                1, \quad \text{if}\ y = f(x_i) \\
                0, \quad \text{otherwise}
              \end{cases}
\end{equation}

It is important to note that $f(x_i)$ is known, that is, it is the class of the sample $x_i$ of the training set.

An obvious variation of the function given in equation \ref{eq:knn_funcao_argmax} is the assignment of weights of each neighbor $k$, according to its distance, to a point $x_q$ being classified \citep{mitchell:97}. This variation implies that points closer to $x_q$ have more influence on its labeling. Formally, we have~\citep{mitchell:97}:

\begin{equation}
\label{eq:knn_funcao_argmax_pesos}
g(x_q) \gets \argmax_{y \in Y} \sum_{i=1}^k w_i \delta (y, f(x_i))
\end{equation}
\noindent where
\begin{equation}
\label{eq:knn_funcao_peso}
  w_i = \frac{1}{d(x_q, x_i)^2}
\end{equation}

In the case where $d(x_q, x_i) = 0$, that is, $x_q$ and $x_i$ are exactly the same coordinates, $g(x_q)$ can assume the same value of $f(x_i)$. If there are other training samples $x_i$ with the same characteristic, then $x_q$ can assume the class of the one who is the most of them \citep{mitchell:97}.



%% ------------------------------------------------------------------------- %%
\subsection{Árvores de decisão}
\label{sec:classifiers_dt}
Árvore de decisão é um método para aproximação de funções alvo discretas, nas quais a função aprendida é representada por uma árvore de decisão ou, ainda, por um conjunto de regras do tipo \emph{Se-Então} que são de fácil interpretação. É uma das técnicas de aprendizagem mais populares de inferência indutiva\footnote{A tarefa de indução é desenvolver uma regra de classificação que pode determinar a classe de qualquer objeto a partir dos valores de seus atributos \citep{quinlan:86}.} \citep{mitchell:97}.

As amostras de um conjunto de dados são classificadas por uma árvore de decisão por um processo iterativo onde um atributo (nó) é escolhido como raiz até algum nó folha, onde a classe é atribuída à amostra. Cada ramo partindo de um nó representa um dos valores possíveis de um dado atributo \citep{mitchell:97}.

Há uma família de algoritmos de árvore de decisão. Um deles é o Divisor Iterativo 3 (ID3) proposto por \citet{quinlan:86}. O ID3 avalia cada atributo através de um teste estatístico para determinar o quão bem ele, por si só, classifica as amostras de treinamento. O melhor atributo é selecionado como o nó raiz da árvore. Um ramo descendente do nó raiz é criado para cada valor possível deste atributo e as amostras de treinamento são classificadas para o nó descendente adequado. Este processo é então repetido recursivamente utilizando as amostras associadas a cada nó descendente. Esse é um algoritmo de busca guloso, já que ele não retrocede para reconsiderar escolhas anteriores \citep{mitchell:97}. O algoritmo para quando o subconjunto de amostras associado a um nó é da mesma classe ou quando não há relevância estatística para uma nova partição \citep{quinlan:86}.

Para medir a impureza de uma partição, o ID3 usa o conceito de entropia, formalmente \citep{quinlan:86}:

\begin{equation}
\label{eq:arvore_devisao_entropia}
H(D) = - y_\oplus log_2 y_\oplus - y_\ominus log_2 y_\ominus
\end{equation}
\noindent onde $D$ é o conjunto de dados de treinamento com $N$ amostras da forma apresentada em \ref{eq:svm_dataset}, $y_\oplus$ e $y_\ominus$ é a proporção de amostras positivas e negativas de $D$, respectivamente. É importante notar que a entropia é 0 quando todas as amostras de $D$ pertencem à mesma classe, 1 quando $D$ contém um número igual de amostras positivas e negativas e um valor entre 0 e 1 nos demais casos \citep{mitchell:97}.

Dada a entropia como uma medida de impureza de um conjunto de amostras de treinamento, pode-se definir o teste estatístico, conhecido como ganho de informação, que mede a efetividade de um atributo na classificação dos dados de treinamento, formalmente \citep{quinlan:86}:
\begin{equation}
\label{eq:arvore_devisao_ganho_informacao}
IG(D, a_r) = H(D) - \sum_{v \in V(a_r)} \frac{|D_v|}{|D|} H(D_v)
\end{equation}
\noindent onde $V(a_r)$ é o conjunto de todos os possíveis valores do atributo $a_r$, $D_v$ é o subconjunto de $D$ no qual o atributo $a_r$ tem o valor $v$.

Vale enfatizar que o primeiro termo da equação dado em \ref{eq:arvore_devisao_ganho_informacao} é a entropia do conjunto original $D$ e o segundo termo é o valor esperado da entropia depois que $D$ é particionado usando o atributo $a_r$ \citep{mitchell:97}. Outro aspecto importante é que o algoritmo inicia com o conjunto $D$ original, que é substituído por $D_v$ à medida que a recursão se aprofunda.

Batizado de C4.5, \citet{quinlan:93} estendeu o ID3 para possibilitar o uso de atributos contínuos, atributos com dados ausentes e melhoria na eficiência computacional. Além disso, essa versão cuida de questões como quão profunda a árvore deve ser para evitar que os dados de treinamento sejam perfeitamente classificados, ou seja, quando o conjunto de treinamento é particionado até que cada subconjunto contenha apenas amostras de uma única classe. A estratégia adotada por \citet{quinlan:93} foi podar a árvore posteriormente à geração da árvore ajustada. Esse processo, apesar de ser mais lento que a proposta anterior do ID3, tornou o algoritmo mais confiável e com maior capacidade de generalização.


%% ------------------------------------------------------------------------- %%